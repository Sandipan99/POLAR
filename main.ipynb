{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from numpy import linalg\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import time\n",
    "from random import shuffle\n",
    "import sys\n",
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load embeddings and normalizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 8.57 s\n"
     ]
    }
   ],
   "source": [
    "model_glove = glove2word2vec('glove.42B.300d.txt','gensim_glove_300d.txt') ## needed for glove embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4min 40s\n"
     ]
    }
   ],
   "source": [
    "model_glove = gensim.models.KeyedVectors.load_word2vec_format(\"gensim_glove_300d.txt\", binary=False) ## needed for only glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.63 ms\n"
     ]
    }
   ],
   "source": [
    "def generate_norm_embedding(model, output_path):\n",
    "    temp_file = open(output_path,'wb')\n",
    "    temp_file.write(str.encode(str(len(model.vocab))+' '+str(model.vector_size)+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.vocab):\n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        temp_file.write(model[each_word]/linalg.norm(model[each_word]))\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "    \n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d87303028246a78a054489a96fee3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1917494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 24.6 s\n"
     ]
    }
   ],
   "source": [
    "generate_norm_embedding(model_glove,'glove_norm_300.mod')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the required word-embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 28.5 s\n"
     ]
    }
   ],
   "source": [
    "model_gn = gensim.models.KeyedVectors.load_word2vec_format('glove_norm_300.mod',binary=True) # ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.71 ms\n"
     ]
    }
   ],
   "source": [
    "current_model = model_gn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 20.1 ms\n"
     ]
    }
   ],
   "source": [
    "list_antonym = []\n",
    "\n",
    "with open('Antonym_sets/LenciBenotto.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "\n",
    "\n",
    "with open('Antonym_sets/LenciBenotto.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('Antonym_sets/EVALution.val') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "with open('Antonym_sets/EVALution.test') as fp:\n",
    "    for line in fp:\n",
    "        parts = line.split()\n",
    "        if parts[3]=='antonym':\n",
    "            word1 = parts[0].split('-')[0]\n",
    "            word2 = parts[1].split('-')[0]\n",
    "            if word1 in current_model and word2 in current_model:\n",
    "                list_antonym.append((word1.strip().lower(), word2.strip().lower()))\n",
    "                \n",
    "                \n",
    "list_antonym = list(dict.fromkeys(list_antonym).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd806fe90e645d4b949702994f0d5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3110), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45123457a1ce4282a95ac7bcccb4c896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b522e0ea51ed4025b350040094ee1047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1468\n",
      "time: 457 ms\n"
     ]
    }
   ],
   "source": [
    "similarity_matrix = defaultdict(list)\n",
    "for each_pair in tqdm(list_antonym):\n",
    "    word1 = each_pair[0]\n",
    "    word2 = each_pair[1]\n",
    "    if word1 < word2:\n",
    "        similarity_matrix[word1].append(word2)\n",
    "    else:\n",
    "        similarity_matrix[word2].append(word1)\n",
    "    \n",
    "all_similarity = defaultdict(dict)\n",
    "for each_key in tqdm(similarity_matrix):\n",
    "    for each_value in similarity_matrix[each_key]:\n",
    "#         cosine_similarity([current_model[each_key]]\n",
    "        all_similarity[each_key][each_value] = abs(cosine_similarity([current_model[each_key]],[current_model[each_value]])[0][0])\n",
    "    \n",
    "final_antonym_list = []\n",
    "for index_counter, each_key in enumerate(tqdm(all_similarity)):\n",
    "#     print(each_key,all_similarity[each_key])\n",
    "    listofTuples = sorted(all_similarity[each_key].items() ,  key=lambda x: x[1])\n",
    "#     print(listofTuples)\n",
    "    final_antonym_list.append((each_key, listofTuples[0][0]))\n",
    "print(len(final_antonym_list))\n",
    "\n",
    "list_antonym = final_antonym_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decide on the size of the antonym vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.69 ms\n"
     ]
    }
   ],
   "source": [
    "num_antonym = 1468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1468, 300)\n",
      "time: 10 ms\n"
     ]
    }
   ],
   "source": [
    "## Find the anatonym difference vectors\n",
    "antonymy_vector = []\n",
    "for each_word_pair in list_antonym:\n",
    "    antonymy_vector.append(current_model[each_word_pair[0]]- current_model[each_word_pair[1]])\n",
    "antonymy_vector = np.array(antonymy_vector)\n",
    "print(antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset Dimension Selection Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db1f944c1fe4fb28b996b606fb48528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from scipy.spatial.distance import cosine as scipy_cosine\n",
    "random.seed(42)\n",
    "\n",
    "t1 = np.array(antonymy_vector)\n",
    "dimension_similarity_matrix = defaultdict(dict)\n",
    "for index_1, each_dim1 in enumerate(tqdm(t1)):\n",
    "    for index_2, each_dim2 in enumerate(t1):\n",
    "        dimension_similarity_matrix[index_1][index_2] = abs(1-scipy_cosine(each_dim1, each_dim2))\n",
    "        \n",
    "        \n",
    "def get_set_score(final_list, each_dim):\n",
    "    final_output = 0.0\n",
    "    for each_vec in final_list:\n",
    "        final_output += dimension_similarity_matrix[each_vec][each_dim]\n",
    "    return final_output/(len(final_list))\n",
    "        \n",
    "def select_subset_dimension(dim_vector, num_dim):\n",
    "    working_list = np.array(dim_vector)\n",
    "    \n",
    "    working_position_index = [i for i in range(working_list.shape[0])]\n",
    "    final_position_index = []\n",
    "    \n",
    "\n",
    "    print('working list is ready, shape', working_list.shape)\n",
    "    sel_dim = random.randrange(0, working_list.shape[0])\n",
    "\n",
    "    final_position_index.append(sel_dim)\n",
    "    \n",
    "    working_position_index.remove(sel_dim)\n",
    "\n",
    "    \n",
    "    for test_count in tqdm(range(num_dim-1)):\n",
    "        min_dim = None\n",
    "        min_score = 1000\n",
    "        for temp_index, each_dim in enumerate(working_position_index):\n",
    "#             print(each_dim)\n",
    "            temp_score = get_set_score(final_position_index, each_dim)\n",
    "            if temp_score< min_score:\n",
    "                min_score= temp_score\n",
    "                min_dim = each_dim\n",
    "        print(test_count,min_dim)\n",
    "        final_position_index.append(min_dim)\n",
    "        working_position_index.remove(min_dim)\n",
    "#         print(working_list.shape, len(final_list))\n",
    "    return final_position_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the ORTHOGONAL DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working list is ready, shape (1468, 300)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587f8498f7254c709d78d0147d76b2a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1442), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 243\n",
      "1 493\n",
      "2 626\n",
      "3 670\n",
      "4 87\n",
      "5 1219\n",
      "6 1156\n",
      "7 485\n",
      "8 1445\n",
      "9 1171\n",
      "10 1433\n",
      "11 258\n",
      "12 1380\n",
      "13 1389\n",
      "14 1462\n",
      "15 1205\n",
      "16 1040\n",
      "17 166\n",
      "18 745\n",
      "19 393\n",
      "20 261\n",
      "21 473\n",
      "22 1122\n",
      "23 1378\n",
      "24 1349\n",
      "25 794\n",
      "26 1174\n",
      "27 1300\n",
      "28 167\n",
      "29 389\n",
      "30 1347\n",
      "31 1262\n",
      "32 460\n",
      "33 1426\n",
      "34 544\n",
      "35 1152\n",
      "36 1226\n",
      "37 1402\n",
      "38 325\n",
      "39 1026\n",
      "40 663\n",
      "41 961\n",
      "42 484\n",
      "43 717\n",
      "44 232\n",
      "45 5\n",
      "46 750\n",
      "47 1108\n",
      "48 488\n",
      "49 290\n",
      "50 1325\n",
      "51 1341\n",
      "52 664\n",
      "53 1311\n",
      "54 1382\n",
      "55 1357\n",
      "56 1418\n",
      "57 343\n",
      "58 1014\n",
      "59 1387\n",
      "60 348\n",
      "61 1003\n",
      "62 1427\n",
      "63 430\n",
      "64 759\n",
      "65 28\n",
      "66 1385\n",
      "67 291\n",
      "68 404\n",
      "69 535\n",
      "70 751\n",
      "71 318\n",
      "72 420\n",
      "73 202\n",
      "74 1305\n",
      "75 695\n",
      "76 944\n",
      "77 129\n",
      "78 920\n",
      "79 11\n",
      "80 67\n",
      "81 1251\n",
      "82 726\n",
      "83 450\n",
      "84 161\n",
      "85 1074\n",
      "86 747\n",
      "87 463\n",
      "88 1290\n",
      "89 1004\n",
      "90 1410\n",
      "91 1237\n",
      "92 1352\n",
      "93 349\n",
      "94 1083\n",
      "95 895\n",
      "96 1274\n",
      "97 146\n",
      "98 592\n",
      "99 221\n",
      "100 1016\n",
      "101 1103\n",
      "102 784\n",
      "103 1400\n",
      "104 529\n",
      "105 1072\n",
      "106 889\n",
      "107 545\n",
      "108 805\n",
      "109 962\n",
      "110 1033\n",
      "111 713\n",
      "112 1020\n",
      "113 418\n",
      "114 771\n",
      "115 1176\n",
      "116 391\n",
      "117 843\n",
      "118 901\n",
      "119 299\n",
      "120 60\n",
      "121 311\n",
      "122 1110\n",
      "123 1\n",
      "124 973\n",
      "125 489\n",
      "126 157\n",
      "127 1210\n",
      "128 644\n",
      "129 818\n",
      "130 1009\n",
      "131 555\n",
      "132 1054\n",
      "133 526\n",
      "134 122\n",
      "135 1444\n",
      "136 1372\n",
      "137 1200\n",
      "138 1465\n",
      "139 924\n",
      "140 1252\n",
      "141 609\n",
      "142 165\n",
      "143 568\n",
      "144 1423\n",
      "145 1221\n",
      "146 932\n",
      "147 216\n",
      "148 313\n",
      "149 1368\n",
      "150 1227\n",
      "151 685\n",
      "152 829\n",
      "153 1018\n",
      "154 975\n",
      "155 679\n",
      "156 814\n",
      "157 846\n",
      "158 537\n",
      "159 1191\n",
      "160 501\n",
      "161 424\n",
      "162 1064\n",
      "163 26\n",
      "164 1336\n",
      "165 655\n",
      "166 1164\n",
      "167 71\n",
      "168 543\n",
      "169 933\n",
      "170 1313\n",
      "171 1089\n",
      "172 770\n",
      "173 691\n",
      "174 1177\n",
      "175 706\n",
      "176 602\n",
      "177 611\n",
      "178 1379\n",
      "179 966\n",
      "180 1253\n",
      "181 1069\n",
      "182 1150\n",
      "183 68\n",
      "184 1053\n",
      "185 996\n",
      "186 245\n",
      "187 113\n",
      "188 125\n",
      "189 534\n",
      "190 1121\n",
      "191 806\n",
      "192 1344\n",
      "193 236\n",
      "194 1127\n",
      "195 1201\n",
      "196 1073\n",
      "197 1338\n",
      "198 776\n",
      "199 904\n",
      "200 824\n",
      "201 1346\n",
      "202 1208\n",
      "203 271\n",
      "204 1261\n",
      "205 731\n",
      "206 657\n",
      "207 287\n",
      "208 378\n",
      "209 1377\n",
      "210 1275\n",
      "211 285\n",
      "212 439\n",
      "213 1063\n",
      "214 451\n",
      "215 606\n",
      "216 677\n",
      "217 421\n",
      "218 1172\n",
      "219 722\n",
      "220 866\n",
      "221 692\n",
      "222 1446\n",
      "223 1019\n",
      "224 715\n",
      "225 1450\n",
      "226 540\n",
      "227 145\n",
      "228 765\n",
      "229 1186\n",
      "230 168\n",
      "231 550\n",
      "232 651\n",
      "233 417\n",
      "234 280\n",
      "235 317\n",
      "236 163\n",
      "237 305\n",
      "238 1079\n",
      "239 796\n",
      "240 269\n",
      "241 834\n",
      "242 475\n",
      "243 359\n",
      "244 710\n",
      "245 62\n",
      "246 1273\n",
      "247 1265\n",
      "248 730\n",
      "249 246\n",
      "250 1415\n",
      "251 594\n",
      "252 1101\n",
      "253 1104\n",
      "254 798\n",
      "255 1031\n",
      "256 453\n",
      "257 1461\n",
      "258 328\n",
      "259 596\n",
      "260 781\n",
      "261 1166\n",
      "262 738\n",
      "263 1440\n",
      "264 459\n",
      "265 754\n",
      "266 1154\n",
      "267 1282\n",
      "268 155\n",
      "269 915\n",
      "270 1218\n",
      "271 1386\n",
      "272 674\n",
      "273 363\n",
      "274 347\n",
      "275 648\n",
      "276 1328\n",
      "277 144\n",
      "278 1187\n",
      "279 197\n",
      "280 946\n",
      "281 1173\n",
      "282 397\n",
      "283 1264\n",
      "284 892\n",
      "285 1090\n",
      "286 408\n",
      "287 1095\n",
      "288 827\n",
      "289 1331\n",
      "290 547\n",
      "291 42\n",
      "292 699\n",
      "293 1161\n",
      "294 1391\n",
      "295 142\n",
      "296 237\n",
      "297 947\n",
      "298 870\n",
      "299 357\n",
      "300 47\n",
      "301 256\n",
      "302 831\n",
      "303 928\n",
      "304 1449\n",
      "305 76\n",
      "306 1241\n",
      "307 589\n",
      "308 621\n",
      "309 711\n",
      "310 331\n",
      "311 906\n",
      "312 1080\n",
      "313 1183\n",
      "314 115\n",
      "315 1165\n",
      "316 273\n",
      "317 1043\n",
      "318 548\n",
      "319 515\n",
      "320 1190\n",
      "321 1330\n",
      "322 1356\n",
      "323 376\n",
      "324 362\n",
      "325 90\n",
      "326 1194\n",
      "327 1048\n",
      "328 993\n",
      "329 495\n",
      "330 308\n",
      "331 149\n",
      "332 1441\n",
      "333 563\n",
      "334 59\n",
      "335 174\n",
      "336 316\n",
      "337 10\n",
      "338 464\n",
      "339 922\n",
      "340 1448\n",
      "341 52\n",
      "342 400\n",
      "343 444\n",
      "344 1196\n",
      "345 105\n",
      "346 496\n",
      "347 1370\n",
      "348 1238\n",
      "349 725\n",
      "350 698\n",
      "351 998\n",
      "352 557\n",
      "353 801\n",
      "354 1403\n",
      "355 327\n",
      "356 867\n",
      "357 1120\n",
      "358 577\n",
      "359 1250\n",
      "360 175\n",
      "361 289\n",
      "362 375\n",
      "363 179\n",
      "364 990\n",
      "365 983\n",
      "366 856\n",
      "367 1085\n",
      "368 894\n",
      "369 868\n",
      "370 553\n",
      "371 900\n",
      "372 1411\n",
      "373 253\n",
      "374 446\n",
      "375 1342\n",
      "376 778\n",
      "377 423\n",
      "378 190\n",
      "379 224\n",
      "380 841\n",
      "381 1416\n",
      "382 1142\n",
      "383 1430\n",
      "384 1413\n",
      "385 1294\n",
      "386 903\n",
      "387 1397\n",
      "388 492\n",
      "389 749\n",
      "390 1112\n",
      "391 1034\n",
      "392 240\n",
      "393 134\n",
      "394 835\n",
      "395 1229\n",
      "396 865\n",
      "397 1168\n",
      "398 629\n",
      "399 82\n",
      "400 1301\n",
      "401 637\n",
      "402 433\n",
      "403 1371\n",
      "404 963\n",
      "405 1113\n",
      "406 1306\n",
      "407 360\n",
      "408 1088\n",
      "409 673\n",
      "410 1179\n",
      "411 791\n",
      "412 816\n",
      "413 1086\n",
      "414 910\n",
      "415 959\n",
      "416 35\n",
      "417 385\n",
      "418 719\n",
      "419 1315\n",
      "420 1025\n",
      "421 1148\n",
      "422 1329\n",
      "423 658\n",
      "424 226\n",
      "425 1277\n",
      "426 412\n",
      "427 388\n",
      "428 1439\n",
      "429 1035\n",
      "430 1303\n",
      "431 917\n",
      "432 32\n",
      "433 301\n",
      "434 622\n",
      "435 697\n",
      "436 1460\n",
      "437 945\n",
      "438 268\n",
      "439 518\n",
      "440 982\n",
      "441 1105\n",
      "442 1419\n",
      "443 265\n",
      "444 332\n",
      "445 934\n",
      "446 833\n",
      "447 539\n",
      "448 774\n",
      "449 1012\n",
      "450 469\n",
      "451 661\n",
      "452 212\n",
      "453 825\n",
      "454 1285\n",
      "455 37\n",
      "456 235\n",
      "457 1233\n",
      "458 1296\n",
      "459 682\n",
      "460 1204\n",
      "461 912\n",
      "462 143\n",
      "463 527\n",
      "464 593\n",
      "465 1058\n",
      "466 1339\n",
      "467 263\n",
      "468 723\n",
      "469 586\n",
      "470 1281\n",
      "471 888\n",
      "472 1270\n",
      "473 716\n",
      "474 608\n",
      "475 721\n",
      "476 1141\n",
      "477 659\n",
      "478 1006\n",
      "479 926\n",
      "480 1099\n",
      "481 1308\n",
      "482 403\n",
      "483 969\n",
      "484 1260\n",
      "485 763\n",
      "486 1078\n",
      "487 352\n",
      "488 1189\n",
      "489 567\n",
      "490 1060\n",
      "491 839\n",
      "492 1170\n",
      "493 476\n",
      "494 757\n",
      "495 1169\n",
      "496 905\n",
      "497 260\n",
      "498 656\n",
      "499 1216\n",
      "500 1363\n",
      "501 1182\n",
      "502 708\n",
      "503 480\n",
      "504 1266\n",
      "505 139\n",
      "506 587\n",
      "507 1100\n",
      "508 1434\n",
      "509 1084\n",
      "510 1432\n",
      "511 768\n",
      "512 186\n",
      "513 241\n",
      "514 1254\n",
      "515 984\n",
      "516 683\n",
      "517 369\n",
      "518 1225\n",
      "519 1087\n",
      "520 8\n",
      "521 1428\n",
      "522 967\n",
      "523 97\n",
      "524 848\n",
      "525 1231\n",
      "526 1070\n",
      "527 617\n",
      "528 671\n",
      "529 935\n",
      "530 1097\n",
      "531 506\n",
      "532 479\n",
      "533 789\n",
      "534 604\n",
      "535 807\n",
      "536 803\n",
      "537 1136\n",
      "538 1184\n",
      "539 1366\n",
      "540 1137\n",
      "541 1280\n",
      "542 103\n",
      "543 552\n",
      "544 994\n",
      "545 981\n",
      "546 1409\n",
      "547 541\n",
      "548 822\n",
      "549 95\n",
      "550 1343\n",
      "551 1335\n",
      "552 732\n",
      "553 1376\n",
      "554 1220\n",
      "555 1425\n",
      "556 244\n",
      "557 766\n",
      "558 578\n",
      "559 81\n",
      "560 986\n",
      "561 1401\n",
      "562 1307\n",
      "563 678\n",
      "564 703\n",
      "565 465\n",
      "566 538\n",
      "567 913\n",
      "568 379\n",
      "569 1098\n",
      "570 1422\n",
      "571 79\n",
      "572 999\n",
      "573 1452\n",
      "574 1464\n",
      "575 1023\n",
      "576 667\n",
      "577 238\n",
      "578 279\n",
      "579 1258\n",
      "580 1068\n",
      "581 436\n",
      "582 925\n",
      "583 724\n",
      "584 63\n",
      "585 1038\n",
      "586 991\n",
      "587 83\n",
      "588 542\n",
      "589 1457\n",
      "590 610\n",
      "591 1133\n",
      "592 1050\n",
      "593 1332\n",
      "594 1384\n",
      "595 1383\n",
      "596 205\n",
      "597 50\n",
      "598 845\n",
      "599 942\n",
      "600 687\n",
      "601 919\n",
      "602 447\n",
      "603 979\n",
      "604 251\n",
      "605 773\n",
      "606 338\n",
      "607 1093\n",
      "608 380\n",
      "609 64\n",
      "610 997\n",
      "611 1295\n",
      "612 405\n",
      "613 853\n",
      "614 623\n",
      "615 1453\n",
      "616 1124\n",
      "617 1209\n",
      "618 96\n",
      "619 522\n",
      "620 1316\n",
      "621 6\n",
      "622 672\n",
      "623 254\n",
      "624 1396\n",
      "625 1405\n",
      "626 882\n",
      "627 1206\n",
      "628 645\n",
      "629 1321\n",
      "630 1257\n",
      "631 1212\n",
      "632 549\n",
      "633 1044\n",
      "634 1288\n",
      "635 521\n",
      "636 688\n",
      "637 1162\n",
      "638 893\n",
      "639 598\n",
      "640 930\n",
      "641 413\n",
      "642 14\n",
      "643 1354\n",
      "644 799\n",
      "645 1369\n",
      "646 264\n",
      "647 330\n",
      "648 650\n",
      "649 109\n",
      "650 1046\n",
      "651 786\n",
      "652 382\n",
      "653 54\n",
      "654 1082\n",
      "655 950\n",
      "656 601\n",
      "657 762\n",
      "658 1304\n",
      "659 1128\n",
      "660 172\n",
      "661 764\n",
      "662 1271\n",
      "663 712\n",
      "664 668\n",
      "665 1146\n",
      "666 1421\n",
      "667 767\n",
      "668 283\n",
      "669 821\n",
      "670 1438\n",
      "671 1367\n",
      "672 1022\n",
      "673 787\n",
      "674 640\n",
      "675 1319\n",
      "676 1375\n",
      "677 219\n",
      "678 1268\n",
      "679 1276\n",
      "680 960\n",
      "681 1286\n",
      "682 860\n",
      "683 1027\n",
      "684 582\n",
      "685 985\n",
      "686 880\n",
      "687 630\n",
      "688 561\n",
      "689 978\n",
      "690 1406\n",
      "691 24\n",
      "692 210\n",
      "693 735\n",
      "694 477\n",
      "695 1163\n",
      "696 936\n",
      "697 27\n",
      "698 1032\n",
      "699 844\n",
      "700 478\n",
      "701 1259\n",
      "702 84\n",
      "703 885\n",
      "704 988\n",
      "705 633\n",
      "706 1314\n",
      "707 1096\n",
      "708 864\n",
      "709 1195\n",
      "710 1324\n",
      "711 532\n",
      "712 1147\n",
      "713 368\n",
      "714 1272\n",
      "715 955\n",
      "716 1355\n",
      "717 736\n",
      "718 871\n",
      "719 1116\n",
      "720 1158\n",
      "721 519\n",
      "722 57\n",
      "723 395\n",
      "724 921\n",
      "725 1420\n",
      "726 294\n",
      "727 702\n",
      "728 93\n",
      "729 434\n",
      "730 1292\n",
      "731 854\n",
      "732 407\n",
      "733 441\n",
      "734 971\n",
      "735 859\n",
      "736 94\n",
      "737 361\n",
      "738 1334\n",
      "739 1322\n",
      "740 1213\n",
      "741 949\n",
      "742 429\n",
      "743 877\n",
      "744 891\n",
      "745 588\n",
      "746 603\n",
      "747 276\n",
      "748 643\n",
      "749 686\n",
      "750 1431\n",
      "751 851\n",
      "752 270\n",
      "753 1407\n",
      "754 275\n",
      "755 1463\n",
      "756 744\n",
      "757 1059\n",
      "758 1192\n",
      "759 1007\n",
      "760 1062\n",
      "761 782\n",
      "762 1155\n",
      "763 585\n",
      "764 1125\n",
      "765 1143\n",
      "766 1039\n",
      "767 878\n",
      "768 523\n",
      "769 1175\n",
      "770 396\n",
      "771 409\n",
      "772 943\n",
      "773 1351\n",
      "774 466\n",
      "775 1353\n",
      "776 896\n",
      "777 223\n",
      "778 247\n",
      "779 1362\n",
      "780 628\n",
      "781 665\n",
      "782 937\n",
      "783 1199\n",
      "784 367\n",
      "785 502\n",
      "786 1056\n",
      "787 58\n",
      "788 511\n",
      "789 599\n",
      "790 206\n",
      "791 1167\n",
      "792 1228\n",
      "793 899\n",
      "794 705\n",
      "795 516\n",
      "796 1451\n",
      "797 156\n",
      "798 193\n",
      "799 74\n",
      "800 649\n",
      "801 1392\n",
      "802 234\n",
      "803 457\n",
      "804 1443\n",
      "805 613\n",
      "806 69\n",
      "807 1140\n",
      "808 4\n",
      "809 647\n",
      "810 306\n",
      "811 1106\n",
      "812 761\n",
      "813 707\n",
      "814 1359\n",
      "815 1153\n",
      "816 230\n",
      "817 437\n",
      "818 298\n",
      "819 419\n",
      "820 964\n",
      "821 119\n",
      "822 625\n",
      "823 1310\n",
      "824 820\n",
      "825 1437\n",
      "826 1447\n",
      "827 1394\n",
      "828 106\n",
      "829 416\n",
      "830 863\n",
      "831 12\n",
      "832 700\n",
      "833 1180\n",
      "834 1399\n",
      "835 1454\n",
      "836 675\n",
      "837 704\n",
      "838 940\n",
      "839 1436\n",
      "840 1256\n",
      "841 696\n",
      "842 908\n",
      "843 972\n",
      "844 939\n",
      "845 627\n",
      "846 862\n",
      "847 680\n",
      "848 1466\n",
      "849 546\n",
      "850 1132\n",
      "851 1077\n",
      "852 1269\n",
      "853 676\n",
      "854 1094\n",
      "855 131\n",
      "856 218\n",
      "857 207\n",
      "858 204\n",
      "859 445\n",
      "860 286\n",
      "861 931\n",
      "862 99\n",
      "863 472\n",
      "864 872\n",
      "865 1318\n",
      "866 329\n",
      "867 785\n",
      "868 1065\n",
      "869 341\n",
      "870 255\n",
      "871 1010\n",
      "872 86\n",
      "873 1297\n",
      "874 406\n",
      "875 886\n",
      "876 1234\n",
      "877 881\n",
      "878 1193\n",
      "879 974\n",
      "880 486\n",
      "881 239\n",
      "882 619\n",
      "883 662\n",
      "884 162\n",
      "885 1456\n",
      "886 1138\n",
      "887 1198\n",
      "888 957\n",
      "889 1036\n",
      "890 1002\n",
      "891 312\n",
      "892 435\n",
      "893 75\n",
      "894 336\n",
      "895 309\n",
      "896 875\n",
      "897 524\n",
      "898 355\n",
      "899 1245\n",
      "900 907\n",
      "901 23\n",
      "902 742\n",
      "903 810\n",
      "904 100\n",
      "905 1243\n",
      "906 295\n",
      "907 217\n",
      "908 191\n",
      "909 1424\n",
      "910 1412\n",
      "911 987\n",
      "912 1091\n",
      "913 180\n",
      "914 1350\n",
      "915 164\n",
      "916 266\n",
      "917 278\n",
      "918 377\n",
      "919 120\n",
      "920 1317\n",
      "921 1348\n",
      "922 31\n",
      "923 188\n",
      "924 1255\n",
      "925 760\n",
      "926 1459\n",
      "927 169\n",
      "928 98\n",
      "929 78\n",
      "930 797\n",
      "931 411\n",
      "932 1223\n",
      "933 181\n",
      "934 194\n",
      "935 826\n",
      "936 788\n",
      "937 842\n",
      "938 1358\n",
      "939 976\n",
      "940 80\n",
      "941 690\n",
      "942 769\n",
      "943 1145\n",
      "944 559\n",
      "945 1323\n",
      "946 1015\n",
      "947 823\n",
      "948 153\n",
      "949 734\n",
      "950 474\n",
      "951 638\n",
      "952 689\n",
      "953 1240\n",
      "954 334\n",
      "955 1278\n",
      "956 481\n",
      "957 151\n",
      "958 458\n",
      "959 1284\n",
      "960 614\n",
      "961 1102\n",
      "962 442\n",
      "963 953\n",
      "964 861\n",
      "965 494\n",
      "966 1041\n",
      "967 499\n",
      "968 415\n",
      "969 455\n",
      "970 66\n",
      "971 1364\n",
      "972 1126\n",
      "973 0\n",
      "974 819\n",
      "975 1115\n",
      "976 884\n",
      "977 1289\n",
      "978 615\n",
      "979 1149\n",
      "980 968\n",
      "981 1442\n",
      "982 583\n",
      "983 927\n",
      "984 858\n",
      "985 1393\n",
      "986 215\n",
      "987 365\n",
      "988 575\n",
      "989 728\n",
      "990 141\n",
      "991 660\n",
      "992 1197\n",
      "993 737\n",
      "994 1249\n",
      "995 1404\n",
      "996 342\n",
      "997 812\n",
      "998 176\n",
      "999 284\n",
      "1000 533\n",
      "1001 233\n",
      "1002 590\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003 182\n",
      "1004 126\n",
      "1005 214\n",
      "1006 514\n",
      "1007 1135\n",
      "1008 504\n",
      "1009 366\n",
      "1010 249\n",
      "1011 452\n",
      "1012 1005\n",
      "1013 646\n",
      "1014 2\n",
      "1015 1320\n",
      "1016 897\n",
      "1017 1017\n",
      "1018 461\n",
      "1019 570\n",
      "1020 958\n",
      "1021 513\n",
      "1022 1123\n",
      "1023 1302\n",
      "1024 1071\n",
      "1025 954\n",
      "1026 1061\n",
      "1027 642\n",
      "1028 293\n",
      "1029 1224\n",
      "1030 310\n",
      "1031 1119\n",
      "1032 128\n",
      "1033 952\n",
      "1034 227\n",
      "1035 1109\n",
      "1036 1340\n",
      "1037 22\n",
      "1038 1066\n",
      "1039 467\n",
      "1040 1345\n",
      "1041 1024\n",
      "1042 225\n",
      "1043 252\n",
      "1044 580\n",
      "1045 326\n",
      "1046 828\n",
      "1047 636\n",
      "1048 800\n",
      "1049 399\n",
      "1050 1222\n",
      "1051 300\n",
      "1052 1381\n",
      "1053 127\n",
      "1054 112\n",
      "1055 482\n",
      "1056 1051\n",
      "1057 471\n",
      "1058 1239\n",
      "1059 847\n",
      "1060 883\n",
      "1061 1390\n",
      "1062 1157\n",
      "1063 1178\n",
      "1064 431\n",
      "1065 319\n",
      "1066 1118\n",
      "1067 1214\n",
      "1068 183\n",
      "1069 1045\n",
      "1070 307\n",
      "1071 503\n",
      "1072 426\n",
      "1073 809\n",
      "1074 1013\n",
      "1075 1244\n",
      "1076 618\n",
      "1077 383\n",
      "1078 733\n",
      "1079 170\n",
      "1080 414\n",
      "1081 51\n",
      "1082 632\n",
      "1083 753\n",
      "1084 681\n",
      "1085 1388\n",
      "1086 43\n",
      "1087 77\n",
      "1088 55\n",
      "1089 852\n",
      "1090 1076\n",
      "1091 500\n",
      "1092 470\n",
      "1093 1337\n",
      "1094 965\n",
      "1095 1021\n",
      "1096 701\n",
      "1097 15\n",
      "1098 373\n",
      "1099 1030\n",
      "1100 941\n",
      "1101 566\n",
      "1102 1360\n",
      "1103 152\n",
      "1104 213\n",
      "1105 454\n",
      "1106 595\n",
      "1107 456\n",
      "1108 916\n",
      "1109 387\n",
      "1110 597\n",
      "1111 780\n",
      "1112 354\n",
      "1113 591\n",
      "1114 1242\n",
      "1115 1111\n",
      "1116 107\n",
      "1117 1374\n",
      "1118 876\n",
      "1119 746\n",
      "1120 714\n",
      "1121 34\n",
      "1122 510\n",
      "1123 600\n",
      "1124 811\n",
      "1125 694\n",
      "1126 1131\n",
      "1127 817\n",
      "1128 1008\n",
      "1129 1267\n",
      "1130 1042\n",
      "1131 1134\n",
      "1132 322\n",
      "1133 1327\n",
      "1134 536\n",
      "1135 135\n",
      "1136 386\n",
      "1137 356\n",
      "1138 631\n",
      "1139 1001\n",
      "1140 195\n",
      "1141 855\n",
      "1142 635\n",
      "1143 39\n",
      "1144 104\n",
      "1145 639\n",
      "1146 351\n",
      "1147 720\n",
      "1148 432\n",
      "1149 956\n",
      "1150 898\n",
      "1151 938\n",
      "1152 790\n",
      "1153 530\n",
      "1154 584\n",
      "1155 315\n",
      "1156 879\n",
      "1157 507\n",
      "1158 390\n",
      "1159 562\n",
      "1160 140\n",
      "1161 49\n",
      "1162 85\n",
      "1163 371\n",
      "1164 1092\n",
      "1165 1246\n",
      "1166 56\n",
      "1167 123\n",
      "1168 977\n",
      "1169 1455\n",
      "1170 9\n",
      "1171 616\n",
      "1172 565\n",
      "1173 573\n",
      "1174 1398\n",
      "1175 137\n",
      "1176 462\n",
      "1177 989\n",
      "1178 1429\n",
      "1179 857\n",
      "1180 1467\n",
      "1181 483\n",
      "1182 61\n",
      "1183 1160\n",
      "1184 428\n",
      "1185 337\n",
      "1186 358\n",
      "1187 1361\n",
      "1188 1283\n",
      "1189 554\n",
      "1190 652\n",
      "1191 528\n",
      "1192 350\n",
      "1193 171\n",
      "1194 1130\n",
      "1195 1417\n",
      "1196 1000\n",
      "1197 117\n",
      "1198 641\n",
      "1199 743\n",
      "1200 124\n",
      "1201 951\n",
      "1202 491\n",
      "1203 345\n",
      "1204 1185\n",
      "1205 1232\n",
      "1206 187\n",
      "1207 1248\n",
      "1208 709\n",
      "1209 1159\n",
      "1210 1299\n",
      "1211 398\n",
      "1212 222\n",
      "1213 520\n",
      "1214 620\n",
      "1215 914\n",
      "1216 159\n",
      "1217 384\n",
      "1218 346\n",
      "1219 1333\n",
      "1220 248\n",
      "1221 333\n",
      "1222 339\n",
      "1223 138\n",
      "1224 560\n",
      "1225 551\n",
      "1226 392\n",
      "1227 321\n",
      "1228 1326\n",
      "1229 422\n",
      "1230 748\n",
      "1231 177\n",
      "1232 108\n",
      "1233 581\n",
      "1234 302\n",
      "1235 150\n",
      "1236 902\n",
      "1237 1055\n",
      "1238 44\n",
      "1239 808\n",
      "1240 579\n",
      "1241 1279\n",
      "1242 813\n",
      "1243 741\n",
      "1244 1207\n",
      "1245 220\n",
      "1246 512\n",
      "1247 666\n",
      "1248 498\n",
      "1249 1298\n",
      "1250 980\n",
      "1251 449\n",
      "1252 48\n",
      "1253 267\n",
      "1254 203\n",
      "1255 250\n",
      "1256 320\n",
      "1257 199\n",
      "1258 13\n",
      "1259 314\n",
      "1260 1144\n",
      "1261 353\n",
      "1262 211\n",
      "1263 259\n",
      "1264 288\n",
      "1265 929\n",
      "1266 201\n",
      "1267 1312\n",
      "1268 1188\n",
      "1269 693\n",
      "1270 185\n",
      "1271 970\n",
      "1272 802\n",
      "1273 1028\n",
      "1274 1151\n",
      "1275 1458\n",
      "1276 558\n",
      "1277 89\n",
      "1278 1075\n",
      "1279 198\n",
      "1280 196\n",
      "1281 7\n",
      "1282 612\n",
      "1283 208\n",
      "1284 17\n",
      "1285 556\n",
      "1286 229\n",
      "1287 101\n",
      "1288 832\n",
      "1289 1052\n",
      "1290 490\n",
      "1291 1107\n",
      "1292 277\n",
      "1293 1435\n",
      "1294 739\n",
      "1295 793\n",
      "1296 840\n",
      "1297 1037\n",
      "1298 53\n",
      "1299 448\n",
      "1300 130\n",
      "1301 41\n",
      "1302 1414\n",
      "1303 304\n",
      "1304 684\n",
      "1305 231\n",
      "1306 178\n",
      "1307 114\n",
      "1308 402\n",
      "1309 836\n",
      "1310 1373\n",
      "1311 281\n",
      "1312 874\n",
      "1313 992\n",
      "1314 438\n",
      "1315 918\n",
      "1316 815\n",
      "1317 29\n",
      "1318 440\n",
      "1319 374\n",
      "1320 887\n",
      "1321 569\n",
      "1322 160\n",
      "1323 755\n",
      "1324 381\n",
      "1325 111\n",
      "1326 718\n",
      "1327 3\n",
      "1328 777\n",
      "1329 508\n",
      "1330 1215\n",
      "1331 571\n",
      "1332 154\n",
      "1333 654\n",
      "1334 19\n",
      "1335 1129\n",
      "1336 727\n",
      "1337 850\n",
      "1338 133\n",
      "1339 401\n",
      "1340 779\n",
      "1341 36\n",
      "1342 1247\n",
      "1343 756\n",
      "1344 38\n",
      "1345 838\n",
      "1346 497\n",
      "1347 1081\n",
      "1348 923\n",
      "1349 158\n",
      "1350 564\n",
      "1351 775\n",
      "1352 73\n",
      "1353 576\n",
      "1354 487\n",
      "1355 292\n",
      "1356 410\n",
      "1357 1029\n",
      "1358 16\n",
      "1359 88\n",
      "1360 758\n",
      "1361 1067\n",
      "1362 65\n",
      "1363 948\n",
      "1364 132\n",
      "1365 1057\n",
      "1366 1293\n",
      "1367 1230\n",
      "1368 792\n",
      "1369 372\n",
      "1370 1395\n",
      "1371 1235\n",
      "1372 1217\n",
      "1373 274\n",
      "1374 91\n",
      "1375 873\n",
      "1376 209\n",
      "1377 729\n",
      "1378 783\n",
      "1379 1291\n",
      "1380 394\n",
      "1381 995\n",
      "1382 1181\n",
      "1383 324\n",
      "1384 837\n",
      "1385 1263\n",
      "1386 184\n",
      "1387 804\n",
      "1388 272\n",
      "1389 344\n",
      "1390 30\n",
      "1391 70\n",
      "1392 20\n",
      "1393 25\n",
      "1394 653\n",
      "1395 795\n",
      "1396 525\n",
      "1397 1202\n",
      "1398 517\n",
      "1399 173\n",
      "1400 297\n",
      "1401 72\n",
      "1402 1047\n",
      "1403 425\n",
      "1404 830\n",
      "1405 192\n",
      "1406 1203\n",
      "1407 1365\n",
      "1408 257\n",
      "1409 572\n",
      "1410 772\n",
      "1411 340\n",
      "1412 228\n",
      "1413 33\n",
      "1414 1114\n",
      "1415 200\n",
      "1416 189\n",
      "1417 1049\n",
      "1418 282\n",
      "1419 468\n",
      "1420 509\n",
      "1421 1236\n",
      "1422 92\n",
      "1423 752\n",
      "1424 262\n",
      "1425 242\n",
      "1426 1011\n",
      "1427 303\n",
      "1428 118\n",
      "1429 335\n",
      "1430 1408\n",
      "1431 605\n",
      "1432 740\n",
      "1433 148\n",
      "1434 1139\n",
      "1435 21\n",
      "1436 624\n",
      "1437 40\n",
      "1438 427\n",
      "1439 669\n",
      "1440 136\n",
      "1441 911\n",
      "\n",
      "(1443,)\n",
      "time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "num_antonym = 1443\n",
    "orthogonal_antonymy_vector =np.array(select_subset_dimension(antonymy_vector, num_antonym))  \n",
    "print(orthogonal_antonymy_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the RANDOM DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1468\n",
      "time: 3.58 ms\n"
     ]
    }
   ],
   "source": [
    "random_antonym_vector = [i for i in range(len(antonymy_vector))]\n",
    "random.shuffle(random_antonym_vector)\n",
    "print(len(random_antonym_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the MAXIMUM VARIANCE DIMENSION Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding size is 1468\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de3d71a2e8bf45598214897801de78c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1917494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 30.7 s\n"
     ]
    }
   ],
   "source": [
    "embedding_size = antonymy_vector.shape[0]   \n",
    "print('The embedding size is', embedding_size)\n",
    "\n",
    "\n",
    "variance_antonymy_vector_inverse = np.linalg.pinv(np.transpose(antonymy_vector))\n",
    "\n",
    "embedding_matrix = []\n",
    "\n",
    "\n",
    "total_words = 0\n",
    "for each_word in tqdm(current_model.vocab):\n",
    "    total_words += 1\n",
    "\n",
    "    new_vector = np.matmul(variance_antonymy_vector_inverse,current_model[each_word])\n",
    "    \n",
    "    embedding_matrix.append(new_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.71 ms\n"
     ]
    }
   ],
   "source": [
    "del new_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327bf89dbf3043fb8548f064c26ea0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1468), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "time: 47.5 s\n"
     ]
    }
   ],
   "source": [
    "variance_list = []\n",
    "\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "\n",
    "for each_dimension in tqdm(range(embedding_matrix.shape[1])):\n",
    "    variance_list.append(np.var(embedding_matrix[:,each_dimension]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.63 ms\n"
     ]
    }
   ],
   "source": [
    "variance_antonymy_vector = [each for each in sorted(range(len(variance_list)), key=lambda i: variance_list[i], reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 211 ms\n"
     ]
    }
   ],
   "source": [
    "del embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.67 ms\n"
     ]
    }
   ],
   "source": [
    "del variance_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformtaion to polar space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4.45 ms\n"
     ]
    }
   ],
   "source": [
    "def transform_to_antonym_space(current_model, output_file_path, binary, current_antonymy_vector_inverse):\n",
    "    embedding_size = current_antonymy_vector_inverse.shape[0]   ##CHANGE THIS ACCORDINGLY!!!\n",
    "    print('New model size is',len(current_model.vocab), embedding_size)\n",
    "\n",
    "    temp_file = None\n",
    "    \n",
    "    if binary:\n",
    "        temp_file = open(output_file_path,'wb')\n",
    "        temp_file.write(str.encode(str(len(current_model.vocab))+' '+str(embedding_size)+'\\n'))\n",
    "    else:\n",
    "        temp_file = open(output_file_path,'w')\n",
    "        temp_file.write(str(len(current_model.vocab))+' '+str(embedding_size)+'\\n')\n",
    "\n",
    "    total_words = 0\n",
    "    for each_word in tqdm(current_model.vocab):\n",
    "        total_words += 1\n",
    "        if binary:\n",
    "            temp_file.write(str.encode(each_word+' '))\n",
    "        else:\n",
    "            temp_file.write(each_word+' ')\n",
    "\n",
    "        new_vector = np.matmul(current_antonymy_vector_inverse,current_model[each_word])\n",
    "\n",
    "        new_vector = new_vector/linalg.norm(new_vector)\n",
    "\n",
    "        \n",
    "        \n",
    "        if binary:\n",
    "            temp_file.write(new_vector)\n",
    "            temp_file.write(str.encode('\\n'))\n",
    "        else:\n",
    "            temp_file.write(str(new_vector))\n",
    "            temp_file.write('\\n')\n",
    "\n",
    "\n",
    "    temp_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard normal transform "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.93 ms\n"
     ]
    }
   ],
   "source": [
    "def standard_normal_dist_model(model, new_filename):\n",
    "    embedding_matrix = []\n",
    "    embedding_vocab = []\n",
    "\n",
    "    temp_file = open(new_filename,'wb')\n",
    "    temp_file.write(str.encode(str(model.vectors.shape[0])+' '+str(model.vectors.shape[1])+'\\n'))\n",
    "    \n",
    "    for each_word in tqdm(model.vocab):\n",
    "        embedding_matrix.append(model[each_word])\n",
    "        embedding_vocab.append(each_word)\n",
    "    \n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    \n",
    "    print('The shape of embedding matrix is {}'.format(embedding_matrix.shape))\n",
    "    \n",
    "    norm_embedding_matrix = (embedding_matrix - embedding_matrix.mean(0))/ embedding_matrix.std(0)\n",
    "    \n",
    "    for word_counter, each_word in enumerate(tqdm(embedding_vocab)):\n",
    "#         assert each_word==embedding_vocab[word_counter],'Not matching!!!'\n",
    "        \n",
    "        temp_file.write(str.encode(each_word+' '))\n",
    "        new_vector = norm_embedding_matrix[word_counter]\n",
    "        temp_file.write(new_vector)\n",
    "        temp_file.write(str.encode('\\n'))\n",
    "        \n",
    "    del embedding_matrix\n",
    "    del embedding_vocab\n",
    "    temp_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the task score for different dimension size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.05 ms\n"
     ]
    }
   ],
   "source": [
    "def generate_embedding_path(current_model, embedding_path, binary, antonym_vector, curr_dim):\n",
    "    curr_antonym_vector = antonymy_vector[antonym_vector[:curr_dim]]\n",
    "    curr_antonymy_vector_inverse = np.linalg.pinv(np.transpose(curr_antonym_vector))\n",
    "    transform_to_antonym_space(current_model, embedding_path, binary,curr_antonymy_vector_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.14 ms\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "dim_size = 500 # Number of POLAR dimenions\n",
    "antonym_vector_method = random_antonym_vector # orthogonal_antonymy_vector, variance_antonymy_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New model size is 1917494 500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d19ec95f0d84296966cd94fd60eef6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1917494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "loading the model\n",
      "loading done..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0887cb27e2b0407282233c74861b57f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1917494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The shape of embedding matrix is (1917494, 500)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "101a35c5c52143ce9d9c67be09e9fa63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1917494), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "b'0.966\\n'\n",
      "finished sentiment task\n",
      "time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "curr_dim = dim_size\n",
    "\n",
    "embedding_path = 'Downstream Task/embeddings/rand_antonym_'+str(curr_dim)+'.bin'\n",
    "generate_embedding_path(current_model, embedding_path,True,random_antonym_vector, curr_dim)\n",
    "\n",
    "print('loading the model')\n",
    "temp_model = gensim.models.KeyedVectors.load_word2vec_format(embedding_path, binary=True)\n",
    "print('loading done..')\n",
    "\n",
    "std_nrml_embedding_path = 'Downstream Task/embeddings/rand_antonym_gl_'+str(curr_dim)+'_StdNrml.bin'\n",
    "standard_normal_dist_model(temp_model, std_nrml_embedding_path)\n",
    "\n",
    "######### Task specific code....\n",
    "command_list = ['python3', \n",
    "                    'Downstream Task/TREC/classify_task.py',\n",
    "                     std_nrml_embedding_path,\n",
    "                     '2',\n",
    "                     'Downstream Task/TREC/data/qa_train_X.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_train_y.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_val_X.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_val_y.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_test_X.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_test_y.pickle'\n",
    "                    ]\n",
    "result = subprocess.run(command_list, stdout=subprocess.PIPE)\n",
    "print(result.stdout)\n",
    "print('finished sentiment task')\n",
    "################\n",
    "del temp_model\n",
    "    \n",
    "os.remove(std_nrml_embedding_path)\n",
    "os.remove(embedding_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code snippet to use for other downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## word analogy....\n",
    "######################################################################   \n",
    "    \n",
    "    del temp_model\n",
    "    \n",
    "    temp_model_nrml = gensim.models.KeyedVectors.load_word2vec_format(std_nrml_embedding_path, binary=True)\n",
    "    accuracy = temp_model_nrml.evaluate_word_analogies(datapath('questions-words.txt'))[0]\n",
    "    \n",
    "    print(accuracy)\n",
    "    final_output_wa.write(str(curr_dim)+'\\tstdnrml\\t')\n",
    "    final_output_wa.write(str(accuracy))\n",
    "    final_output_wa.write('\\n')\n",
    "    final_output_wa.flush()\n",
    "    \n",
    "    del temp_model_nrml\n",
    "######################################################################\n",
    "### Sentiment classification    \n",
    "    command_list = ['python3', \n",
    "                    'Downstream Task/sentiment/classify_sentiment.py',\n",
    "                     std_nrml_embedding_path,\n",
    "                     '2',\n",
    "                     'Downstream Task/sentiment/data/sentiment_train_X.p',\n",
    "                     'Downstream Task/sentiment/data/sentiment_train_y.p',\n",
    "                     'Downstream Task/sentiment/data/sentiment_val_X.p',\n",
    "                     'Downstream Task/sentiment/data/sentiment_val_y.p',\n",
    "                     'Downstream Task/sentiment/data/sentiment_test_X.p',\n",
    "                     'Downstream Task/sentiment/data/sentiment_test_y.p'\n",
    "                    ]\n",
    "    result = subprocess.run(command_list, stdout=subprocess.PIPE)\n",
    "    print(result.stdout)\n",
    "    print('finished sentiment task')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "### Question classification\n",
    "    command_list = ['python3', \n",
    "                    'Downstream Task/TREC/classify_task.py',\n",
    "                     std_nrml_embedding_path,\n",
    "                     '2',\n",
    "                     'Downstream Task/TREC/data/qa_train_X.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_train_y.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_val_X.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_val_y.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_test_X.pickle',\n",
    "                     'Downstream Task/TREC/data/qa_test_y.pickle'\n",
    "                    ]\n",
    "    result = subprocess.run(command_list, stdout=subprocess.PIPE)\n",
    "    print(result.stdout)\n",
    "    print('finished TREC task')\n",
    "    \n",
    "\n",
    "#####################################################################\n",
    "### Discriminative attribute identification\n",
    "    command_list = ['python3', \n",
    "                    'Downstream Task/Discrim_Attr/classify_discrim_attr_TASK.py',\n",
    "                    std_nrml_embedding_path,\n",
    "                    '2',\n",
    "                    'Downstream Task/Discrim_Attr/data/discrim_attr_train_X.p',\n",
    "                    'Downstream Task/Discrim_Attr/data/discrim_attr_train_y.p',\n",
    "                    'Downstream Task/Discrim_Attr/data/discrim_attr_val_X.p',\n",
    "                    'Downstream Task/Discrim_Attr/data/discrim_attr_val_y.p',\n",
    "                    'Downstream Task/Discrim_Attr/data/discrim_attr_test_X.p',\n",
    "                    'Downstream Task/Discrim_Attr/data/discrim_attr_test_y.p'\n",
    "                   ]\n",
    "    result = subprocess.run(command_list, stdout=subprocess.PIPE)\n",
    "    print(result.stdout)\n",
    "    print('finished Discr task')\n",
    "    \n",
    "######################################################################\n",
    "### Computer news classification\n",
    "    command_list = ['python3',\n",
    "                    'Downstream Task/newsgroups/classify.py',\n",
    "                    std_nrml_embedding_path,\n",
    "                    '2',\n",
    "                    'Downstream Task/newsgroups/data/news_computer_train_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_computer_train_y.p',\n",
    "                    'Downstream Task/newsgroups/data/news_computer_val_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_computer_val_y.p',\n",
    "                    'Downstream Task/newsgroups/data/news_computer_test_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_computer_test_y.p'\n",
    "    ]\n",
    "    result = subprocess.run(command_list, stdout=subprocess.PIPE)\n",
    "    print(result.stdout)\n",
    "    print('finished compter nc task')\n",
    "\n",
    "######################################################################   \n",
    "### Religious news classification\n",
    "    command_list = ['python3',\n",
    "                    'Downstream Task/newsgroups/classify.py',\n",
    "                    std_nrml_embedding_path,\n",
    "                    '2',\n",
    "                    'Downstream Task/newsgroups/data/news_religion_train_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_religion_train_y.p',\n",
    "                    'Downstream Task/newsgroups/data/news_religion_val_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_religion_val_y.p',\n",
    "                    'Downstream Task/newsgroups/data/news_religion_test_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_religion_test_y.p'\n",
    "    ]\n",
    "    result = subprocess.run(command_list, stdout=subprocess.PIPE)\n",
    "    print(result.stdout)\n",
    "    print('finished Religion nc task')\n",
    "\n",
    "######################################################################\n",
    "### sports news classification\n",
    "    command_list = ['python3',\n",
    "                    'Downstream Task/newsgroups/classify.py',\n",
    "                    std_nrml_embedding_path,\n",
    "                    '2',\n",
    "                    'Downstream Task/newsgroups/data/news_sports_train_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_sports_train_y.p',\n",
    "                    'Downstream Task/newsgroups/data/news_sports_val_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_sports_val_y.p',\n",
    "                    'Downstream Task/newsgroups/data/news_sports_test_X.p',\n",
    "                    'Downstream Task/newsgroups/data/news_sports_test_y.p'\n",
    "    ]\n",
    "    result = subprocess.run(command_list, stdout=subprocess.PIPE)\n",
    "    print(result.stdout)\n",
    "    print('finished Sports nc task')\n",
    "\n",
    "    \n",
    "######################################################################  \n",
    "### noun phrase bracketing\n",
    "    res = []\n",
    "    for _ in range(10):\n",
    "        command_list = ['python3',\n",
    "                        'Downstream Task/np_bracketing/classify_bracketing.py',\n",
    "                        std_nrml_embedding_path,\n",
    "                        '2',\n",
    "                        'Downstream Task/np_bracketing/data/npbracketing_train_X'+str(_)+'.pickle',\n",
    "                        'Downstream Task/np_bracketing/data/npbracketing_train_y'+str(_)+'.pickle',\n",
    "                        'Downstream Task/np_bracketing/data/npbracketing_val_X'+str(_)+'.pickle',\n",
    "                        'Downstream Task/np_bracketing/data/npbracketing_val_y'+str(_)+'.pickle',\n",
    "                        'Downstream Task/np_bracketing/data/npbracketing_test_X'+str(_)+'.pickle',\n",
    "                        'Downstream Task/np_bracketing/data/npbracketing_test_y'+str(_)+'.pickle'\n",
    "        ]\n",
    "\n",
    "        result = subprocess.run(command_list, stdout=subprocess.PIPE)\n",
    "        print(result.stdout)\n",
    "        res.append(float(result.stdout.decode('utf-8').strip()))\n",
    "    \n",
    "    #print(res)\n",
    "    m_s = np.mean(res)\n",
    "    print(m_s)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
